{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéµ Spectral Affinity: Neural AI Pipeline (MERT)\n",
                "\n",
                "This version upgrades the \"Hearing\" system to use **MERT (Music Semantic Audio Transformer)**. \n",
                "Instead of mathematical formulas, it uses a Neural Network pre-trained on millions of songs to understand musical context.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "!pip install librosa soundfile tqdm joblib scikit-learn transformers torchaudio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Imports & AI Model Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import glob\n",
                "import shutil\n",
                "import pathlib\n",
                "import warnings\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "import torchaudio\n",
                "import torchaudio.transforms as T\n",
                "from transformers import Wav2Vec2FeatureExtractor, AutoModel\n",
                "from sklearn.preprocessing import StandardScaler, normalize\n",
                "from sklearn.cluster import AffinityPropagation, KMeans as skKMeans\n",
                "from IPython.display import FileLink\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "try:\n",
                "    import cuml\n",
                "    from cuml.cluster import KMeans as cuKMeans\n",
                "    HAS_CUML = True\n",
                "except ImportError:\n",
                "    HAS_CUML = False\n",
                "\n",
                "print(\"üß† Loading MERT-v1-95M AI Model...\")\n",
                "MODEL_ID = \"m-a-p/MERT-v1-95M\"\n",
                "processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
                "model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True).to(device)\n",
                "model.eval()\n",
                "print(f\"‚úÖ Model loaded on {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Neural Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_filename(filename):\n",
                "    if '.' not in filename: return filename\n",
                "    name_body, ext = filename.rsplit('.', 1)\n",
                "    prefixes = [r\"^Slavic-\", r\"^Theme_OST-\", r\"^My_Workspace-\", r\"^audio-\"]\n",
                "    for prefix in prefixes: name_body = re.sub(prefix, \"\", name_body, flags=re.IGNORECASE)\n",
                "    uuid_pattern = r\"[\\(\\.\\-_\\s]?[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}[\\)]?$\"\n",
                "    name_body = re.sub(uuid_pattern, \"\", name_body)\n",
                "    name_body = name_body.replace(\"_\", \" \").strip(\" -(_)\")\n",
                "    name_body = re.sub(r\"\\s+\", \" \", name_body).strip()\n",
                "    return f\"{name_body if name_body else 'Unnamed'}.{ext}\"\n",
                "\n",
                "def get_ai_embeddings(file_path, duration=15): # 15s is enough for MERT to get the vibe\n",
                "    try:\n",
                "        # Load Audio\n",
                "        info = torchaudio.info(file_path)\n",
                "        sr = info.sample_rate\n",
                "        total_frames = info.num_frames\n",
                "        \n",
                "        # Crop center\n",
                "        target_frames = duration * sr\n",
                "        if total_frames > target_frames:\n",
                "            start_frame = (total_frames - target_frames) // 2\n",
                "            waveform, _ = torchaudio.load(file_path, frame_offset=start_frame, num_frames=target_frames)\n",
                "        else:\n",
                "            waveform, _ = torchaudio.load(file_path)\n",
                "            \n",
                "        # Resample to 24k (MERT Native)\n",
                "        if sr != 24000:\n",
                "            resampler = T.Resample(sr, 24000).to(waveform.device)\n",
                "            waveform = resampler(waveform)\n",
                "            \n",
                "        # Mix to mono\n",
                "        if waveform.shape[0] > 1:\n",
                "            waveform = waveform.mean(dim=0, keepdim=True)\n",
                "            \n",
                "        # Prepare input\n",
                "        input_values = processor(waveform.squeeze().numpy(), sampling_rate=24000, return_tensors=\"pt\").input_values.to(device)\n",
                "        \n",
                "        # Inference\n",
                "        with torch.no_grad():\n",
                "            outputs = model(input_values)\n",
                "            # Use the last hidden state averaged over time\n",
                "            hidden_states = outputs.last_hidden_state\n",
                "            embeddings = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
                "            \n",
                "        return embeddings\n",
                "\n",
                "    except Exception as e:\n",
                "        return f\"ERROR: {str(e)}\"\n",
                "\n",
                "def organize_files(file_paths, rel_dest_paths, output_dir):\n",
                "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
                "    for i, file_path in enumerate(tqdm(file_paths, desc=\"üìÅ Organizing\", leave=True)):\n",
                "        full_dest = os.path.join(output_dir, rel_dest_paths[i])\n",
                "        pathlib.Path(os.path.dirname(full_dest)).mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        name_pure, ext = os.path.splitext(full_dest)\n",
                "        final_path = full_dest\n",
                "        counter = 1\n",
                "        while os.path.exists(final_path):\n",
                "            final_path = f\"{name_pure} ({counter}){ext}\"\n",
                "            counter += 1\n",
                "        shutil.copy2(file_path, final_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Neural Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CONFIGURATION ---\n",
                "INPUT_DIR = \"/kaggle/input/datasets/danieldobles/ost-songs\"\n",
                "OUTPUT_DIR = \"/kaggle/working/organized_music\"\n",
                "N_CONTINENTS = 5\n",
                "ISLAND_SENSITIVITY = 0.8 # Higher = More sub-clusters\n",
                "CLEAN_NAMES = True\n",
                "# ---------------------\n",
                "\n",
                "print(\"üéµ --- SPECTRAL AFFINITY: MERT NEURAL PIPELINE ---\")\n",
                "\n",
                "print(\"üîç Step 1/4: Mapping library...\")\n",
                "audio_extensions = ['*.mp3', '*.wav', '*.flac', '*.ogg', '*.m4a']\n",
                "file_paths = []\n",
                "for ext in audio_extensions:\n",
                "    file_paths.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n",
                "    file_paths.extend(glob.glob(os.path.join(INPUT_DIR, \"**\", ext), recursive=True))\n",
                "file_paths = list(set(file_paths))\n",
                "print(f\"‚úÖ Found {len(file_paths)} files.\")\n",
                "\n",
                "if not file_paths:\n",
                "    print(\"‚ùå ERROR: Nothing to process!\")\n",
                "else:\n",
                "    print(\"\\nüß† Step 2/4: Neural Listening (MERT AI on GPU)...\")\n",
                "    results = []\n",
                "    errors = []\n",
                "    \n",
                "    # Sequential GPU inference is usually fast enough for MERT-95M\n",
                "    for p in tqdm(file_paths, desc=\"üéß Embedding\", leave=True):\n",
                "        res = get_ai_embeddings(p)\n",
                "        if isinstance(res, str) and res.startswith(\"ERROR:\"):\n",
                "            errors.append(res)\n",
                "            results.append(None)\n",
                "        else:\n",
                "            results.append(res)\n",
                "    \n",
                "    valid_embeddings = [r for r in results if r is not None]\n",
                "    valid_paths = [p for r, p in zip(results, file_paths) if r is not None]\n",
                "    \n",
                "    if valid_embeddings:\n",
                "        # Normalize embeddings for Cosine Similarity behavior with KMeans\n",
                "        X_global = normalize(np.array(valid_embeddings).astype('float32'))\n",
                "        \n",
                "        print(f\"\\nüåç Step 3/4: Continent Clustering... {'(GPU)' if HAS_CUML else '(CPU)'}\")\n",
                "        if HAS_CUML:\n",
                "            primary_labels = cuKMeans(n_clusters=N_CONTINENTS).fit_predict(X_global)\n",
                "        else:\n",
                "            primary_labels = skKMeans(n_clusters=N_CONTINENTS, n_init=10).fit_predict(X_global)\n",
                "        \n",
                "        cluster_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
                "        final_rel_paths = [\"\" for _ in valid_paths]\n",
                "        \n",
                "        print(\"\\nüèñÔ∏è Step 4/4: Deep Semantic Affinity (Islands)...\")\n",
                "        for c_idx in range(N_CONTINENTS):\n",
                "            indices = [i for i, l in enumerate(primary_labels) if l == c_idx]\n",
                "            if not indices: continue\n",
                "            \n",
                "            c_letter = cluster_letters[c_idx % 26]\n",
                "            X_sub = X_global[indices]\n",
                "            \n",
                "            # Affinity Propagation finds distinct 'exemplars'\n",
                "            try:\n",
                "                af = AffinityPropagation(damping=ISLAND_SENSITIVITY, random_state=42).fit(X_sub)\n",
                "                sub_labels = af.labels_\n",
                "                n_subs = len(set(sub_labels))\n",
                "            except: \n",
                "                # Fallback if AF fails on strange data\n",
                "                sub_labels = [0] * len(indices)\n",
                "                n_subs = 1\n",
                "            \n",
                "            for i, local_idx in enumerate(indices):\n",
                "                s_label = sub_labels[i]\n",
                "                filename = os.path.basename(valid_paths[local_idx])\n",
                "                if CLEAN_NAMES: filename = clean_filename(filename)\n",
                "                \n",
                "                if n_subs > 1: \n",
                "                    # Hierarchical Path: Cluster_A/Sub_1\n",
                "                    sub_path = os.path.join(f\"Cluster_{c_letter}\", f\"Sub_{s_label + 1}\")\n",
                "                    prefixed_name = f\"[{c_letter}-{s_label + 1}] {filename}\"\n",
                "                    final_rel_paths[local_idx] = os.path.join(sub_path, prefixed_name)\n",
                "                else:\n",
                "                    # Flat Path: Cluster_A\n",
                "                    final_rel_paths[local_idx] = os.path.join(f\"Cluster_{c_letter}\", f\"[{c_letter}] {filename}\")\n",
                "        \n",
                "        organize_files(valid_paths, final_rel_paths, OUTPUT_DIR)\n",
                "        print(f\"\\n‚ú® SUCCESS: Semantic Organization Complete! Check {OUTPUT_DIR}\")\n",
                "    else:\n",
                "        print(\"‚ùå ERROR: Neural extraction failed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Final Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(OUTPUT_DIR):\n",
                "    print(\"üì¶ Packaging results (ZIP)...\")\n",
                "    !zip -0 -rq /kaggle/working/organized_results.zip organized_music\n",
                "    print(\"‚úÖ Generation complete!\")\n",
                "    display(FileLink('organized_results.zip'))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (RAPIDS)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
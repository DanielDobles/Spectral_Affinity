{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "dc8e701a",
            "metadata": {},
            "source": [
                "# üéµ Spectral Affinity: Neural AI Pipeline (MERT)\n",
                "\n",
                "This specialized version uses **MERT (Music Semantic Audio Transformer)** to understand your music with neural intelligence.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1684a67b",
            "metadata": {},
            "source": [
                "### 1. Environment & Performance Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65e127c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "\n",
                "# üõ°Ô∏è Silencing the noise\n",
                "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
                "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import torch\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"üöÄ System ready. Device: {device}\")\n",
                "\n",
                "with open(os.devnull, 'w') as fnull:\n",
                "    import sys\n",
                "    # Temporarily hide pip output for a clean look\n",
                "    orig_stdout = sys.stdout\n",
                "    sys.stdout = fnull\n",
                "    !pip install -q librosa soundfile tqdm joblib scikit-learn transformers torchaudio\n",
                "    sys.stdout = orig_stdout\n",
                "\n",
                "print(\"‚úÖ Dependencies verified.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f9b75ee",
            "metadata": {},
            "source": [
                "### 2. Imports & Neural Brain Loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ece2a188",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import glob\n",
                "import shutil\n",
                "import pathlib\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "import torchaudio\n",
                "import torchaudio.transforms as T\n",
                "from transformers import Wav2Vec2FeatureExtractor, AutoModel, logging as hf_logging\n",
                "from sklearn.preprocessing import StandardScaler, normalize\n",
                "from sklearn.cluster import AffinityPropagation, KMeans as skKMeans\n",
                "from IPython.display import FileLink\n",
                "\n",
                "# Set transformers to stay quiet\n",
                "hf_logging.set_verbosity_error()\n",
                "\n",
                "try:\n",
                "    import cuml\n",
                "    from cuml.cluster import KMeans as cuKMeans\n",
                "    HAS_CUML = True\n",
                "except ImportError:\n",
                "    HAS_CUML = False\n",
                "\n",
                "print(\"üß† Activating Neural Brain (MERT-AI)...\", end=\" \", flush=True)\n",
                "MODEL_ID = \"m-a-p/MERT-v1-95M\"\n",
                "\n",
                "try:\n",
                "    # Load with trust_remote_code and silence the warning via logging\n",
                "    processor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
                "    model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True).to(device)\n",
                "    model.eval()\n",
                "    print(\"Done.\")\n",
                "    print(f\"‚úÖ MERT-v1 activated on {device}\")\n",
                "except Exception as e:\n",
                "    print(\"\\n‚ö†Ô∏è Error loading AI model. Falling back to CPU mode.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "271af7bb",
            "metadata": {},
            "source": [
                "### 3. Logic & Processing Engine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26e7e8dc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_filename(filename):\n",
                "    if '.' not in filename: return filename\n",
                "    name_body, ext = filename.rsplit('.', 1)\n",
                "    prefixes = [r\"^Slavic-\", r\"^Theme_OST-\", r\"^My_Workspace-\", r\"^audio-\"]\n",
                "    for prefix in prefixes: name_body = re.sub(prefix, \"\", name_body, flags=re.IGNORECASE)\n",
                "    uuid_pattern = r\"[\\(\\.\\-_\\s]?[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}[\\)]?$\"\n",
                "    name_body = re.sub(uuid_pattern, \"\", name_body)\n",
                "    name_body = name_body.replace(\"_\", \" \").strip(\" -(_)\")\n",
                "    name_body = re.sub(r\"\\s+\", \" \", name_body).strip()\n",
                "    return f\"{name_body if name_body else 'Unnamed'}.{ext}\"\n",
                "\n",
                "def get_ai_embeddings(file_path, duration=15):\n",
                "    try:\n",
                "        info = torchaudio.info(file_path)\n",
                "        sr = info.sample_rate\n",
                "        total_frames = info.num_frames\n",
                "        \n",
                "        target_frames = duration * sr\n",
                "        if total_frames > target_frames:\n",
                "            start_frame = (total_frames - target_frames) // 2\n",
                "            waveform, _ = torchaudio.load(file_path, frame_offset=start_frame, num_frames=target_frames)\n",
                "        else:\n",
                "            waveform, _ = torchaudio.load(file_path)\n",
                "            \n",
                "        if sr != 24000:\n",
                "            resampler = T.Resample(sr, 24000).to(waveform.device)\n",
                "            waveform = resampler(waveform)\n",
                "            \n",
                "        if waveform.shape[0] > 1:\n",
                "            waveform = waveform.mean(dim=0, keepdim=True)\n",
                "            \n",
                "        input_values = processor(waveform.squeeze().numpy(), sampling_rate=24000, return_tensors=\"pt\").input_values.to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model(input_values)\n",
                "            hidden_states = outputs.last_hidden_state\n",
                "            embeddings = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
                "            \n",
                "        return embeddings\n",
                "    except Exception: return None\n",
                "\n",
                "def organize_files(file_paths, rel_dest_paths, output_dir):\n",
                "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
                "    for i, file_path in enumerate(tqdm(file_paths, desc=\"üìÅ Organizing\", leave=True)):\n",
                "        full_dest = os.path.join(output_dir, rel_dest_paths[i])\n",
                "        pathlib.Path(os.path.dirname(full_dest)).mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        name_pure, ext = os.path.splitext(full_dest)\n",
                "        final_path = full_dest\n",
                "        counter = 1\n",
                "        while os.path.exists(final_path):\n",
                "            final_path = f\"{name_pure} ({counter}){ext}\"\n",
                "            counter += 1\n",
                "        shutil.copy2(file_path, final_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b4550b0b",
            "metadata": {},
            "source": [
                "### 4. Sequential Execution Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8ce9e96",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- SETTINGS ---\n",
                "INPUT_DIR = \"/kaggle/input/datasets/danieldobles/ost-songs\"\n",
                "OUTPUT_DIR = \"/kaggle/working/organized_music\"\n",
                "N_CONTINENTS = 5\n",
                "ISLAND_SENSITIVITY = 0.8\n",
                "CLEAN_NAMES = True\n",
                "# ----------------\n",
                "\n",
                "print(\"üéµ --- SPECTRAL AFFINITY: NEURAL CLASSIFIER ---\")\n",
                "\n",
                "print(\"üîç Step 1/4: Mapping library...\")\n",
                "audio_extensions = ['*.mp3', '*.wav', '*.flac', '*.ogg', '*.m4a']\n",
                "file_paths = []\n",
                "for ext in audio_extensions:\n",
                "    file_paths.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n",
                "    file_paths.extend(glob.glob(os.path.join(INPUT_DIR, \"**\", ext), recursive=True))\n",
                "file_paths = list(set(file_paths))\n",
                "print(f\"‚úÖ Done: Found {len(file_paths)} files.\")\n",
                "\n",
                "if not file_paths:\n",
                "    print(\"‚ùå ERROR: Nothing found!\")\n",
                "else:\n",
                "    print(\"\\nüß† Step 2/4: Neural Extraction (Processing sonic signatures)...\")\n",
                "    results = []\n",
                "    for p in tqdm(file_paths, desc=\"üéß Analysis\", leave=True):\n",
                "        results.append(get_ai_embeddings(p))\n",
                "    \n",
                "    valid_embeddings = [r for r in results if r is not None]\n",
                "    valid_paths = [p for r, p in zip(results, file_paths) if r is not None]\n",
                "    \n",
                "    if valid_embeddings:\n",
                "        X_global = normalize(np.array(valid_embeddings).astype('float32'))\n",
                "        \n",
                "        print(f\"\\nüåç Step 3/4: Continent Clustering (Global Affinity)... {'(GPU)' if HAS_CUML else '(CPU)'}\")\n",
                "        if HAS_CUML:\n",
                "            p_labels = cuKMeans(n_clusters=N_CONTINENTS).fit_predict(X_global)\n",
                "        else:\n",
                "            p_labels = skKMeans(n_clusters=N_CONTINENTS, n_init=10).fit_predict(X_global)\n",
                "        \n",
                "        cluster_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
                "        final_rel_paths = [\"\" for _ in valid_paths]\n",
                "        \n",
                "        print(\"\\nüèñÔ∏è Step 4/4: Deep Similarity Subdivision (Creating sonic islands)...\")\n",
                "        for c_idx in range(N_CONTINENTS):\n",
                "            indices = [i for i, l in enumerate(p_labels) if l == c_idx]\n",
                "            if not indices: continue\n",
                "            \n",
                "            c_letter = cluster_letters[c_idx % 26]\n",
                "            X_sub = X_global[indices]\n",
                "            \n",
                "            try:\n",
                "                af = AffinityPropagation(damping=ISLAND_SENSITIVITY, random_state=42).fit(X_sub)\n",
                "                sub_labels = af.labels_\n",
                "                n_subs = len(set(sub_labels))\n",
                "            except: \n",
                "                sub_labels = [0] * len(indices)\n",
                "                n_subs = 1\n",
                "            \n",
                "            for i, local_idx in enumerate(indices):\n",
                "                s_label = sub_labels[i]\n",
                "                filename = os.path.basename(valid_paths[local_idx])\n",
                "                if CLEAN_NAMES: filename = clean_filename(filename)\n",
                "                if n_subs > 1: \n",
                "                    sub_path = os.path.join(f\"Cluster_{c_letter}\", f\"Sub_{s_label + 1}\")\n",
                "                    prefac = f\"[{c_letter}-{s_label + 1}] {filename}\"\n",
                "                    final_rel_paths[local_idx] = os.path.join(sub_path, prefac)\n",
                "                else:\n",
                "                    final_rel_paths[local_idx] = os.path.join(f\"Cluster_{c_letter}\", f\"[{c_letter}] {filename}\")\n",
                "        \n",
                "        organize_files(valid_paths, final_rel_paths, OUTPUT_DIR)\n",
                "        print(f\"\\n‚ú® SUCCESS: Your library has been organized with AI.\")\n",
                "    else:\n",
                "        print(\"‚ùå ERROR: Processing failed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2269d148",
            "metadata": {},
            "source": [
                "### 5. Final Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "981061db",
            "metadata": {},
            "outputs": [],
            "source": [
                "if os.path.exists(OUTPUT_DIR):\n",
                "    !zip -0 -rq /kaggle/working/organized_results.zip organized_music\n",
                "    print(\"‚úÖ ZIP Bundle generated successfully.\")\n",
                "    display(FileLink('organized_results.zip'))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (RAPIDS)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}


import json
import os

NOTEBOOK_PATH = r"c:\Users\armon\DEV_main\Spectral_Affinity\Spectral_Affinity_Master.ipynb"

# 1. New Source for SpectralMasterEngine (Analysis)
# Optimized to load with torchaudio (faster) and parallelize loading
NEW_ANALYSIS_SOURCE = [
    "class SpectralMasterEngine:\n",
    "    def __init__(self, device='cuda', sr=24000, cache_file='spectral_master_cache.json'):\n",
    "        self.device, self.sr, self.cache_file = device, sr, cache_file\n",
    "        self.cache = json.load(open(cache_file)) if os.path.exists(cache_file) else {}\n",
    "        self.cqt = CQT1992v2(sr=sr, n_bins=84, bins_per_octave=12).to(device)\n",
    "        major = torch.tensor([6.35,2.23,3.48,2.33,4.38,4.09,2.52,5.19,2.39,3.66,2.29,2.88], device=device)\n",
    "        minor = torch.tensor([6.33,2.68,3.52,5.38,2.60,3.53,2.54,4.75,3.98,2.69,3.34,3.17], device=device)\n",
    "        self.profiles = torch.stack([torch.roll(major,i) for i in range(12)] + [torch.roll(minor,i) for i in range(12)]).t()\n",
    "        print('ðŸ§  Loading MERT...')\n",
    "        self.proc = Wav2Vec2FeatureExtractor.from_pretrained('m-a-p/MERT-v1-95M', trust_remote_code=True)\n",
    "        self.mert = AutoModel.from_pretrained('m-a-p/MERT-v1-95M', trust_remote_code=True).to(device).eval()\n",
    "\n",
    "    def save_cache(self): json.dump(self.cache, open(self.cache_file, 'w'))\n",
    "\n",
    "    def get_camelot(self, key, mode):\n",
    "        cm = {('B','major'):'01B',('F#','major'):'02B',('C#','major'):'03B',('G#','major'):'04B',\n",
    "              ('D#','major'):'05B',('A#','major'):'06B',('F','major'):'07B',('C','major'):'08B',\n",
    "              ('G','major'):'09B',('D','major'):'10B',('A','major'):'11B',('E','major'):'12B',\n",
    "              ('G#','minor'):'01A',('D#','minor'):'02A',('A#','minor'):'03A',('F','minor'):'04A',\n",
    "              ('C','minor'):'05A',('G','minor'):'06A',('D','minor'):'07A',('A','minor'):'08A',\n",
    "              ('E','minor'):'09A',('B','minor'):'10A',('F#','minor'):'11A',('C#','minor'):'12A'}\n",
    "        return cm.get((key, mode.lower()), '00X')\n",
    "\n",
    "    def process_batch(self, paths_batch):\n",
    "        # ðŸš€ GPU OPTIMIZED: Parallel Loading using torchaudio\n",
    "        pc = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
    "        \n",
    "        def load_one(p):\n",
    "            try:\n",
    "                w, s = torchaudio.load(p)\n",
    "                if s != self.sr: w = T.Resample(s, self.sr)(w)\n",
    "                w = w.mean(0) # mono for analysis\n",
    "                # Cut or pad to 120s max for analysis to save VRAM\n",
    "                if w.shape[0] > self.sr*120: w = w[:self.sr*120]\n",
    "                return w, len(w)/self.sr\n",
    "            except: return None\n",
    "\n",
    "        # Load on CPU threads\n",
    "        audios, durs, valid_paths = [], [], []\n",
    "        with ThreadPoolExecutor() as pool:\n",
    "            results = list(pool.map(load_one, paths_batch))\n",
    "            \n",
    "        for i, res in enumerate(results):\n",
    "            if res:\n",
    "                audios.append(res[0]); durs.append(res[1]); valid_paths.append(paths_batch[i])\n",
    "        \n",
    "        if not audios: return []\n",
    "\n",
    "        # Batch to GPU\n",
    "        max_len = max([a.shape[0] for a in audios])\n",
    "        t = torch.zeros(len(audios), max_len, device=self.device)\n",
    "        for i, a in enumerate(audios): t[i, :a.shape[0]] = a.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            spec = self.cqt(t)\n",
    "            energy = spec.pow(2).mean(dim=(1,2)).cpu().numpy()\n",
    "            # Chroma energy normalized\n",
    "            chroma = spec.view(len(audios),7,12,-1).sum(dim=(1,3))\n",
    "            chroma = chroma / (chroma.norm(dim=1,keepdim=True)+1e-6)\n",
    "            best = torch.argmax(torch.matmul(chroma, self.profiles), dim=1).cpu().numpy()\n",
    "            \n",
    "            # Embeddings (One by one to save VRAM on MERT)\n",
    "            embs = []\n",
    "            for i in range(len(audios)):\n",
    "                mid = audios[i].shape[0]//2; sl = int(self.sr*15)\n",
    "                s = audios[i][mid:mid+sl].cpu().numpy() if audios[i].shape[0]>sl else audios[i].cpu().numpy()\n",
    "                iv = self.proc(s, sampling_rate=self.sr, return_tensors='pt').input_values.to(self.device)\n",
    "                embs.append(self.mert(iv).last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist())\n",
    "\n",
    "        results_data = []\n",
    "        for i in range(len(audios)):\n",
    "            # Quick BPM on CPU section (first 60s)\n",
    "            try:\n",
    "                 s_bpm = audios[i][:self.sr*60].cpu().numpy()\n",
    "                 bpm = float(librosa.beat.tempo(onset_envelope=librosa.onset.onset_strength(y=s_bpm, sr=self.sr), sr=self.sr, aggregate=np.mean)[0])\n",
    "            except: bpm = 120.0\n",
    "            \n",
    "            r = {'key':pc[best[i]%12], 'mode':'major' if best[i]<12 else 'minor', \n",
    "                 'energy':float(energy[i]), 'duration':float(durs[i]), \n",
    "                 'embedding':embs[i], 'path':valid_paths[i], 'bpm':bpm}\n",
    "            r['camelot'] = self.get_camelot(r['key'], r['mode'])\n",
    "            results_data.append(r)\n",
    "        return results_data"
]

# 2. New Execution Source (Execution)
# Optimized with async mastering
NEW_EXEC_SOURCE = [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  CONFIG â€” SLAVIC SONGS v6.3 (GPU OPTIMIZED)                  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "INPUT_DIR       = '/kaggle/input/datasets/danieldobles/slavic-songs'\n",
    "REFERENCE_TRACK = '/kaggle/input/datasets/danieldobles/slavic-songs/REF.flac'\n",
    "OUTPUT_DIR      = '/kaggle/working/master_organized'\n",
    "TEMP_DIR        = '/kaggle/working/_temp_restore'\n",
    "BATCH_SIZE, SET_DUR, N_CLUSTERS = 16, 75*60, 3\n",
    "\n",
    "STAGES = {\n",
    "    'neural_clean':    True,   # 1. DFN3\n",
    "    'spectral_shape':  True,   # 2. Stabilizer\n",
    "    'phase_shape':     True,   # 3. Phase Shaper\n",
    "    'stereo_widen':    True,   # 4. Stereo Wider\n",
    "    'mono_bass':       True,   # 5. Sub mono\n",
    "    'transient_punch': True,   # 6. Punch\n",
    "    'spectre_restore': True,   # 7. HF Exciter\n",
    "    'matchering':      True,   # 9. Mastering Match\n",
    "}\n",
    "\n",
    "# â”€â”€ ðŸŽ›ï¸ STABILIZER â”€â”€\n",
    "SHAPER = {'amount': 60, 'speed': 90, 'sensitivity': 30, 'focus_low': 200, 'focus_high': 16000}\n",
    "\n",
    "# â”€â”€ ðŸŒ€ PHASE SHAPER â”€â”€\n",
    "PHASE_CONTROL = -0.3    # -1.0 (tight/linear) â† 0.0 (bypass) â†’ +1.0 (analog/warm)\n",
    "\n",
    "# â”€â”€ ðŸ”€ STEREO WIDER â”€â”€\n",
    "STEREO_WIDTH  = 0.3     # 0.0 (no change) to 1.0 (max width)\n",
    "\n",
    "# â”€â”€ Other â”€â”€\n",
    "PUNCH_DB, BASS_HZ = 4.0, 150\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print('\\nðŸ”§ Initializing 9-Stage Optimized Pipeline...')\n",
    "analyzer  = SpectralMasterEngine(device=device, sr=24000)\n",
    "restorer  = UltimateSunoMaster(device=device, stages=STAGES)\n",
    "mastering = MasteringEngine(ref=REFERENCE_TRACK)\n",
    "\n",
    "# â”€â”€ Phase 1: Scan & Analyze (Optimized) â”€â”€\n",
    "print('\\nðŸ” Scanning...')\n",
    "paths = sorted(set(sum([glob.glob(os.path.join(INPUT_DIR,'**',e), recursive=True) for e in ['*.mp3','*.wav','*.flac','*.m4a']], [])))\n",
    "print(f'   {len(paths)} tracks found')\n",
    "\n",
    "to_do = [p for p in paths if p not in analyzer.cache]\n",
    "if to_do:\n",
    "    print(f'   Analyzing {len(to_do)} new tracks...')\n",
    "    # Now passing PATHS directly to analyzer which handles threading\n",
    "    for ci in range(0, len(to_do), BATCH_SIZE):\n",
    "        chunk = to_do[ci:ci+BATCH_SIZE]\n",
    "        results = analyzer.process_batch(chunk)\n",
    "        for r in results: analyzer.cache[r['path']]=r\n",
    "        analyzer.save_cache(); gc.collect()\n",
    "\n",
    "# â”€â”€ Phase 2: Cluster & Sequence â”€â”€\n",
    "library = [analyzer.cache[p] for p in paths if p in analyzer.cache]\n",
    "if not library: print('âŒ No files')\n",
    "else:\n",
    "    X = normalize(np.array([t['embedding'] for t in library]))\n",
    "    labels = (cuKMeans(n_clusters=N_CLUSTERS) if HAS_CUML else skKMeans(n_clusters=N_CLUSTERS, n_init=10)).fit_predict(X)\n",
    "    clusters = {i: [library[j] for j,l in enumerate(labels) if l==i] for i in range(N_CLUSTERS)}\n",
    "    if os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "    # â”€â”€ Phase 3: Async Restore & Master â”€â”€\n",
    "    print('\\nðŸŒˆ Processing with v6.3 Async Pipeline...')\n",
    "    \n",
    "    # Async Master Worker\n",
    "    master_pool = ThreadPoolExecutor(max_workers=1)\n",
    "    futures = []\n",
    "    \n",
    "    def run_master(in_p, out_p):\n",
    "        try:\n",
    "            if mastering.available: mastering.master(in_p, out_p); os.remove(in_p)\n",
    "            else: shutil.move(in_p, out_p)\n",
    "        except Exception as e: print(f'Master error: {e}')\n",
    "\n",
    "    total, failed = 0, 0\n",
    "    for ci, ct in clusters.items():\n",
    "        cn = f'Group_{chr(65+ci)}'; pool = sorted(ct, key=lambda x: x['energy']); si = 1\n",
    "        while pool:\n",
    "            oset = sequence_chromatic_set(pool, SET_DUR)\n",
    "            if not oset: break\n",
    "            pool = [t for t in pool if t['path'] not in {s['path'] for s in oset}]\n",
    "            sd = os.path.join(OUTPUT_DIR, cn, f'Set_{si}'); os.makedirs(sd, exist_ok=True)\n",
    "            \n",
    "            for i, t in enumerate(tqdm(oset, desc=f'ðŸ’Ž {cn} Set {si}', leave=False)):\n",
    "                on = f\"{str(i+1).zfill(2)} - [{t['camelot']} - {int(t['bpm'])}BPM] {clean_name(t['path'])}.flac\"\n",
    "                fp, tp = os.path.join(sd, on), os.path.join(TEMP_DIR, f'tmp_{ci}_{si}_{i}.wav')\n",
    "                try:\n",
    "                    w, s = torchaudio.load(t['path']); w = restorer._to_48k(w.to(device), s)\n",
    "                    if STAGES.get('neural_clean'): w = restorer.neural_clean(w)\n",
    "                    if STAGES.get('spectral_shape'): w = restorer.spectral_shape(w, **SHAPER)\n",
    "                    if STAGES.get('phase_shape'): w = restorer.phase_shape(w, control=PHASE_CONTROL)\n",
    "                    if STAGES.get('stereo_widen'): w = restorer.stereo_widen(w, width=STEREO_WIDTH)\n",
    "                    if STAGES.get('mono_bass'): w = restorer.mono_bass(w, BASS_HZ)\n",
    "                    if STAGES.get('transient_punch'): w = restorer.transient_punch(w, PUNCH_DB)\n",
    "                    if STAGES.get('spectre_restore'): w = restorer.spectre_restore(w)\n",
    "                    \n",
    "                    # Save temp for mastering\n",
    "                    torchaudio.save(tp, w.cpu(), 48000, encoding='PCM_S', bits_per_sample=16)\n",
    "                    \n",
    "                    # Offload mastering to CPU thread so GPU can start next track immediately\n",
    "                    futures.append(master_pool.submit(run_master, tp, fp))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f'  âš ï¸ {e}'); shutil.copy2(t['path'], fp); failed += 1\n",
    "                total += 1; gc.collect(); torch.cuda.empty_cache()\n",
    "            si += 1\n",
    "    \n",
    "    # Wait for all mastering to finish\n",
    "    print('â³ Finalizing mastering...')\n",
    "    for f in tqdm(futures, desc='Mastering'): f.result()\n",
    "    master_pool.shutdown()\n",
    "    \n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "    print(f'\\nâœ… {total} tracks processed ({failed} failed)')\n",
    "    !zip -0 -rq SlavMaster_v6_3.zip master_organized\n",
    "    display(HTML(\"<h3>ðŸš€ <a href='SlavMaster_v6_3.zip' id='dl'>DOWNLOAD v6.3 MASTER</a></h3>\"))\n",
    "    display(HTML(\"<script>setTimeout(()=>document.getElementById('dl').click(),1000)</script>\"))"
]

def main():
    with open(NOTEBOOK_PATH, 'r', encoding='utf-8') as f:
        nb = json.load(f)
        
    modified = False
    for cell in nb['cells']:
        if cell.get('id') == 'analysis':
            print("Found analysis cell, optimizing...")
            cell['source'] = NEW_ANALYSIS_SOURCE
            modified = True
        elif cell.get('id') == 'exec':
            print("Found exec cell, optimizing...")
            cell['source'] = NEW_EXEC_SOURCE
            modified = True
            
    if modified:
        with open(NOTEBOOK_PATH, 'w', encoding='utf-8') as f:
            json.dump(nb, f, indent=4, ensure_ascii=False)
        print("Notebook optimized successfully!")
    else:
        print("Could not find target cells to optimize.")

if __name__ == "__main__":
    main()
